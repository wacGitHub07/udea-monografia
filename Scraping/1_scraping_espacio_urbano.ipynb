{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para ejecutar el proceso de extracción de datos del sítio https://www.espaciourbano.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota Importante!!!:\n",
    "El proceso a continuacion describe los pasos seguidos para la obtención de la información por scraping de un sitio activo que oferta inmuebles en arriendo en la ciudad de Medellín, esto quiere decir que al ejecutarlo en fechas diferentes es posible que no se genere la misma información dadas las dinamicas de este negocio.\n",
    "\n",
    "**Los datos utilizados para el modelo se encuentran el la carpera _datos_modelo_**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de archivos en utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de listado de users agents\n",
    "user_agets = pd.read_csv(\"utils/user-agent.csv\")\n",
    "\n",
    "# Carga de listado de proxies\n",
    "with open(\"utils/proxies.json\") as json_file:\n",
    "    proxies = json.load(json_file)\n",
    "proxies = proxies.get(\"proxies\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definiciones de URLs**\n",
    "\n",
    "El sitio espacio urbano contiene 5 zonas para casas en alquiler en la ciudad de Medellín:\n",
    "- Centro\n",
    "- Poblado\n",
    "- Belen\n",
    "- Laureles\n",
    "- San Antonio de Prado\n",
    "\n",
    "Cada una de las zonas contiene una URL producto de interactuar con el sitio y hacer los respectivos filtros por zonas, además cada una de estas puede tener resultados en n páginas, estas urls se definen a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Principal\n",
    "head_url = \"https://www.espaciourbano.com/\"\n",
    "\n",
    "# URL Zona Centro\n",
    "urls_zona_centro   = [\"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=1&nCiudad=Medellin%20Zona%201%20-%20Centro\",          # Página 1\n",
    "                      \"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=1&nCiudad=Medellin+Zona+1+%2D+Centro&offset={pages}\"] # Página n\n",
    "\n",
    "# URL Zona Poblado\n",
    "urls_poblado = [\"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10027&pTipoInmueble=1&nCiudad=Medellin%20Zona%202%20-%20El%20Poblado\",         # Página 1\n",
    "                \"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10027&pTipoInmueble=1&nCiudad=Medellin+Zona+2+%2D+El+Poblado&offset={pages}\"]  # Página n\n",
    "\n",
    "# URL Zona Belen\n",
    "urls_belen = [\"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10029&pTipoInmueble=1&nCiudad=Medellin%20Zona%204%20-%20Belen\",          # Página 1\n",
    "              \"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10029&pTipoInmueble=1&nCiudad=Medellin+Zona+4+%2D+Belen&offset={pages}\"] # Página n\n",
    "\n",
    "# URL Laureles\n",
    "urls_laureles = [\"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10028&pTipoInmueble=1&nCiudad=Medellin%20Zona%203%20-%20Laureles\",          # Página 1\n",
    "                 \"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10028&pTipoInmueble=1&nCiudad=Medellin+Zona+3+%2D+Laureles&offset={pages}\"] # Página n\n",
    "\n",
    "# URL San Antonio de Prado\n",
    "urls_san_antonio = [\"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10041&pTipoInmueble=1&nCiudad=San%20Antonio%20de%20Prado\",          # Página 1\n",
    "                    \"https://www.espaciourbano.com/resumen_ciudad_arriendos.asp?pCiudad=10041&pTipoInmueble=1&nCiudad=San+Antonio+de+Prado&offset={pages}\"] # Página n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones:**\n",
    "\n",
    "La información de cada uno de los inmuebles esta dividida en\n",
    "- Precio\n",
    "- Zona\n",
    "- Comonidades\n",
    "- Caracteristicas\n",
    "\n",
    "Se define una función que realiza la petición a cada sitio y extrae la información de cada inmueble con el respectivo código de registro de la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties_info(url: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Función para realizar la petición a una url especifica\n",
    "    y devolver los datos de cada inmueble\n",
    "\n",
    "    PARAMS\n",
    "        url: str\n",
    "            url de una página con el listado de los inmuenles en arriendo\n",
    "    RETURN\n",
    "        (cod, lease) tuple\n",
    "            código de un inmueble con su respectivo grupos de valores\n",
    "            precio, zona, comodidades y caracteristicas\n",
    "    \"\"\"\n",
    "    # Variar el user agent por petición\n",
    "    headers = {'User-Agent': user_agets.sample(1)[\"user-agent\"].values[0]}\n",
    "\n",
    "    # Consulta y carga de la url de detalle\n",
    "    detail = requests.get(url.replace(\"Ficha\",\"ficha\"), headers=headers)\n",
    "\n",
    "    soup_detail =BeautifulSoup(detail.content,'lxml')\n",
    "\n",
    "    # Obtención del código de vivienda, precio y zona\n",
    "    price_zone = soup_detail.find_all('div', attrs={'class':'text-center'})\n",
    "    price_zone = price_zone[1].find_all('h3')\n",
    "    code = soup_detail.find_all('div', attrs={'class':'text-center'})[0].text.strip()\n",
    "    price = price_zone[0].text.replace(\"\\r\\n\",\"\").strip()\n",
    "    zone = price_zone[2].text\n",
    "\n",
    "    # Obtención de las comodidades\n",
    "    comforts = []\n",
    "    comforts_div = soup_detail.find('div', attrs={'class' : 'col-lg-4'}).find_all('p')\n",
    "    for com in comforts_div:\n",
    "        comfort = com.text.strip()\n",
    "        if comfort != \"\": \n",
    "            comforts.append(comfort)\n",
    "\n",
    "    \n",
    "    # Obtención de las características\n",
    "    characteristics = {}\n",
    "    characteristics_div = soup_detail.find('div', attrs={'class' : 'col-lg-8'}).find('table').find_all('tr')\n",
    "    for cha in characteristics_div:\n",
    "        row_cha = cha.find_all('td')\n",
    "        characteristics[row_cha[0].text.strip()] = row_cha[1].text.strip()\n",
    "\n",
    "    lease = {\n",
    "        \"precio\" : price,\n",
    "        \"zona\" : zone,\n",
    "        \"comodidades\" : comforts,\n",
    "        \"caracteristicas\" : characteristics\n",
    "    }\n",
    "\n",
    "    # retorno de valores\n",
    "    return code, lease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data: set, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Función para imprimir datos en un archivo .txt\n",
    "    PARAMS\n",
    "        data: set\n",
    "            Conjunto con los datos a imprimir en el archivo\n",
    "        file_name: str\n",
    "            Nombre del archivo a crear\n",
    "    RETUN\n",
    "    \n",
    "    \"\"\"\n",
    "    # Transfomación de los datos a una lista\n",
    "    data = list(data)\n",
    "    # Crear archivo sobre el cual se va a escribir\n",
    "    with open(f'{file_name}.txt', 'w') as f:\n",
    "        # Ciclo para escribir cada línea en el archivo\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_info(urls: list,\n",
    "                  pages: int,\n",
    "                  path: int,\n",
    "                  file_name: str,\n",
    "                  zona: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Función para consultar la información de los inmuebles por cada zona\n",
    "    PARAMS\n",
    "        urls:lis\n",
    "            Lista de las urls por zona\n",
    "        pages: int\n",
    "            Cantidad de páginas que tiene cada zona\n",
    "        path: int\n",
    "            Salto de página de cada sitio\n",
    "        file_name: str\n",
    "            Nombre del archivo donde se almacenan los resultados\n",
    "        zona: str\n",
    "            Nombre de zona que se esta procesando\n",
    "    RETURN\n",
    "        leases: dict\n",
    "            Información de los inmuebles por zona\n",
    "        characteristics: set\n",
    "            Consolidado de los tipos de características por zona\n",
    "        comforts: set\n",
    "            Consolidado de las diferentes comodidades por zona\n",
    "    \"\"\"\n",
    "\n",
    "    # Diccionario para almacenar los resultados\n",
    "    leases = {}\n",
    "    # Conjuntos para almacenar los consolidados de\n",
    "    # Comodidades y características\n",
    "    characteristics = set()\n",
    "    comforts = set()\n",
    "\n",
    "    print(f\"Consultando Zona: {zona}\")\n",
    "\n",
    "    for i in range(0, pages):\n",
    "        \n",
    "        user_agets.sample(1)[\"user-agent\"].values[0]\n",
    "        # Se selecciona un agente de forma aleatoria\n",
    "        headers = {'User-Agent': user_agets.sample(1)[\"user-agent\"].values[0]}\n",
    "        \n",
    "        # Selección de la página a consultar\n",
    "        if i == 0:\n",
    "            url = urls[0]\n",
    "        else:\n",
    "            url = urls[1].format(pages = i*path)\n",
    "\n",
    "        print(f\"-- Consultando página : {i + 1}\")\n",
    "        response = requests.get(url,headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Por medio de BeautifulSoup se extrae el contendio de la pagina \n",
    "            soup_list = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "            # Cargar todas las clases div con para obtener el detalle de las viviendas\n",
    "            urls_details = soup_list.find_all('div', attrs={'class' : 'col-sm-4'})\n",
    "            urls_details.pop(0)\n",
    "            for url_detail in urls_details:\n",
    "                a = url_detail.find('a')\n",
    "                url = head_url + a.get('href')\n",
    "                code, lease = get_properties_info(url)\n",
    "                leases[code] = lease\n",
    "                characteristics = characteristics.union(set(lease[\"caracteristicas\"].keys()))\n",
    "                comforts = comforts.union(set(lease[\"comodidades\"]))\n",
    "        else:\n",
    "            print(\"ERROR: Falla consultando url: {}\".format(url))\n",
    "    \n",
    "    # Almacenado en archivos\n",
    "    print(\"Imprimiendo Resultados...\")\n",
    "    with open(f\"{file_name}.json\", \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(leases, outfile)\n",
    "    write_file(characteristics, file_name + \"_carac\")\n",
    "    write_file(comforts, file_name + \"_com\")\n",
    "    print(\"Scraping Completo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ejecución de proceso de peticiones o requests:***\n",
    "\n",
    "Teniendo las urls y las funciones se procede a ejecutar el proceso de extracción de información por cada zona, en este proceso se deben generar 3 archivos:\n",
    "- .json: archivo json por zona con la informacion de los inmuebles\n",
    "- .txt: archivo con el consolidado de las características de todos los inmuebles de cada zona\n",
    "- .txt: archivo con el consolidado de las comodidades de todos los inmuebles de cada zona\n",
    "\n",
    "Los archivos de comodidades y características se deben generar para crear una variable booleana con cada uno de los valores de estos archivos, por tanto por cada inmueble se tendrá una variable booleana que indique si tiene o no dicha caracteristica\n",
    "\n",
    "Estos resultados serán almacenados en la carpeta _resultados_request_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping: Zona Centro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "urls = urls_zona_centro\n",
    "pages = 5 # Número de páginas del sitio para esta zona\n",
    "path = 50 # Indica el salto de página\n",
    "file_name = \"resultados_request/zona_1_centro\"\n",
    "characteristics_file = \"resultados_request/zona_1_centro_carac\"\n",
    "comforts_file = \"resultados_request/zona_1_centro_com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando Zona: Centro\n",
      "-- Consultando página : 1\n",
      "-- Consultando página : 2\n",
      "-- Consultando página : 3\n",
      "-- Consultando página : 4\n",
      "-- Consultando página : 5\n",
      "Imprimiendo Resultados...\n",
      "Scraping Completo\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del scraping\n",
    "get_zone_info(urls,\n",
    "              pages,\n",
    "              path,\n",
    "              file_name,\n",
    "              zona = \"Centro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping: Zona Poblado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "urls = urls_poblado\n",
    "pages = 38 # Número de páginas del sitio para esta zona\n",
    "path = 50 # Indica el salto de página\n",
    "file_name = \"resultados_request/zona_2_poblado\"\n",
    "characteristics_file = \"resultados_request/zona_2_poblado_carac\"\n",
    "comforts_file = \"resultados_request/zona_2_poblado_com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando Zona: Poblado\n",
      "-- Consultando página : 1\n",
      "-- Consultando página : 2\n",
      "-- Consultando página : 3\n",
      "-- Consultando página : 4\n",
      "-- Consultando página : 5\n",
      "-- Consultando página : 6\n",
      "-- Consultando página : 7\n",
      "-- Consultando página : 8\n",
      "-- Consultando página : 9\n",
      "-- Consultando página : 10\n",
      "-- Consultando página : 11\n",
      "-- Consultando página : 12\n",
      "-- Consultando página : 13\n",
      "-- Consultando página : 14\n",
      "-- Consultando página : 15\n",
      "-- Consultando página : 16\n",
      "-- Consultando página : 17\n",
      "-- Consultando página : 18\n",
      "-- Consultando página : 19\n",
      "-- Consultando página : 20\n",
      "-- Consultando página : 21\n",
      "-- Consultando página : 22\n",
      "-- Consultando página : 23\n",
      "-- Consultando página : 24\n",
      "-- Consultando página : 25\n",
      "-- Consultando página : 26\n",
      "-- Consultando página : 27\n",
      "-- Consultando página : 28\n",
      "-- Consultando página : 29\n",
      "-- Consultando página : 30\n",
      "-- Consultando página : 31\n",
      "-- Consultando página : 32\n",
      "-- Consultando página : 33\n",
      "-- Consultando página : 34\n",
      "-- Consultando página : 35\n",
      "-- Consultando página : 36\n",
      "-- Consultando página : 37\n",
      "-- Consultando página : 38\n",
      "Imprimiendo Resultados...\n",
      "Scraping Completo\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del scraping\n",
    "get_zone_info(urls,\n",
    "              pages,\n",
    "              path,\n",
    "              file_name,\n",
    "              zona = \"Poblado\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping: Zona Belén**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "urls = urls_belen\n",
    "pages = 8 # Número de páginas del sitio para esta zona\n",
    "path = 50 # Indica el salto de página\n",
    "file_name = \"resultados_request/zona_3_belen\"\n",
    "characteristics_file = \"resultados_request/zona_3_belen_carac\"\n",
    "comforts_file = \"resultados_request/zona_3_belen_com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando Zona: Belén\n",
      "-- Consultando página : 1\n",
      "-- Consultando página : 2\n",
      "-- Consultando página : 3\n",
      "-- Consultando página : 4\n",
      "-- Consultando página : 5\n",
      "-- Consultando página : 6\n",
      "-- Consultando página : 7\n",
      "-- Consultando página : 8\n",
      "Imprimiendo Resultados...\n",
      "Scraping Completo\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del scraping\n",
    "get_zone_info(urls,\n",
    "              pages,\n",
    "              path,\n",
    "              file_name,\n",
    "              zona = \"Belén\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping: Zona Laureles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "urls = urls_laureles\n",
    "pages = 19 # Número de páginas del sitio para esta zona\n",
    "path = 50 # Indica el salto de página\n",
    "file_name = \"resultados_request/zona_4_laureles\"\n",
    "characteristics_file = \"resultados_request/zona_4_laureles_carac\"\n",
    "comforts_file = \"resultados_request/zona_4_laureles_com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando Zona: Laureles\n",
      "-- Consultando página : 1\n",
      "-- Consultando página : 2\n",
      "-- Consultando página : 3\n",
      "-- Consultando página : 4\n",
      "-- Consultando página : 5\n",
      "-- Consultando página : 6\n",
      "-- Consultando página : 7\n",
      "-- Consultando página : 8\n",
      "-- Consultando página : 9\n",
      "-- Consultando página : 10\n",
      "-- Consultando página : 11\n",
      "-- Consultando página : 12\n",
      "-- Consultando página : 13\n",
      "-- Consultando página : 14\n",
      "-- Consultando página : 15\n",
      "-- Consultando página : 16\n",
      "-- Consultando página : 17\n",
      "-- Consultando página : 18\n",
      "-- Consultando página : 19\n",
      "Imprimiendo Resultados...\n",
      "Scraping Completo\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del scraping\n",
    "get_zone_info(urls,\n",
    "              pages,\n",
    "              path,\n",
    "              file_name,\n",
    "              zona = \"Laureles\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping: Zona San Antonio de Prado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "urls = urls_san_antonio\n",
    "pages = 2 # Número de páginas del sitio para esta zona\n",
    "path = 50 # Indica el salto de página\n",
    "file_name = \"resultados_request/zona_5_san_antonio\"\n",
    "characteristics_file = \"resultados_request/zona_5_san_antonio_carac\"\n",
    "comforts_file = \"resultados_request/zona_5_san_antonio_com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando Zona: San Antonio de Prado\n",
      "-- Consultando página : 1\n",
      "-- Consultando página : 2\n",
      "Imprimiendo Resultados...\n",
      "Scraping Completo\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del scraping\n",
    "get_zone_info(urls,\n",
    "              pages,\n",
    "              path,\n",
    "              file_name,\n",
    "              zona = \"San Antonio de Prado\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procesado de los datos**\n",
    "\n",
    "Al obtener los resultados del scraping y el consolidado de las caracteristicas y comodidades se deben organizar los datos de manera tabulada para construir la base de entrenamiento del modelo. Este proceso de consiste en:\n",
    "- Por zona crear un archivo csv cuyas columnas esten formateadas sin acentos ni caracteres especiales que afecten su lectura\n",
    "- Las variables obtenidas se dividen en 3 grupos\n",
    "    - general: variables como: precio, area\n",
    "    - caracteristicas: variables como: tipo pisos, número de niveles\n",
    "    - comodidades: variables como: zona de ropas, jacuzzi, red de gas\n",
    "- Por cada caracteristica del consolidado se crea una variable booleana para indicar si el inmueble cuenta o no con esa característica\n",
    "- La comodidades obtenidas se convierten en variables categoricas numéricas, estas son las que expresan con una cantidad las ventajas de una vivienda, por ejemplo, numero de baños, habitaciones, etc\n",
    "- El restante de las variables serán numéricas o categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_data_scraping(data: dict, format_columns: list, columns: dict, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Funcion para procesar y tabular los datos obtenidos por el proceso de web scraping\n",
    "    PARAMS\n",
    "        data: dict\n",
    "            Diccionario con los datos obtenidos por el proceso de web scraping\n",
    "        format_columns: list\n",
    "            Listado de columnas formateadas\n",
    "        colums: dict\n",
    "            Diccionario separado por grupo de variables consolidadas\n",
    "        file_name: str\n",
    "            Nombre del archivo donde se almacenan los resultados\n",
    "            \n",
    "    \"\"\"\n",
    "    data_frame_zona = pd.DataFrame(columns=format_columns)\n",
    "\n",
    "    viviendas = list(data.keys())\n",
    "    row = {}\n",
    "    print(\"Formateando datos...\")\n",
    "    for vivienda in viviendas:\n",
    "        dat = data[vivienda]\n",
    "        # Lectura de variables generales\n",
    "        row[\"codigo\"] = vivienda\n",
    "        row[\"precio\"] = dat[\"precio\"]\n",
    "        row[\"zona\"] = dat[\"zona\"]\n",
    "\n",
    "        # Variables\n",
    "        for car in columns[\"caracteristicas\"].keys():\n",
    "            row[columns[\"caracteristicas\"][car]] = dat[\"caracteristicas\"][car]\n",
    "\n",
    "        for com in columns[\"comodidades\"].keys():\n",
    "            if com in dat[\"comodidades\"]:\n",
    "                row[columns[\"comodidades\"][com]] = [1]\n",
    "            else:\n",
    "                row[columns[\"comodidades\"][com]] = [0]\n",
    "        \n",
    "        data_frame_zona = pd.concat([data_frame_zona,pd.DataFrame.from_dict(row)])\n",
    "\n",
    "    data_frame_zona.to_csv(file_name, index=False)\n",
    "    print(\"Formato completo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codigo', 'precio', 'zona', 'barrio_sector', 'tipo_pisos', 'baños_familiares', 'area_bruta', 'ciudad', 'numero_niveles', 'tipo_cocina', 'parqueaderos', 'otras_comodidades', 'alcobas_familiares', 'estrato', 'area_total', 'juegos_infantiles', 'balcon', 'zona_ropas', 'camaras_cctv', 'cancha_polideportiva', 'ascensor', 'cancha_squash', 'zona_bbq', 'patio', 'unidad_cerrada_conjunto', 'zonas_verdes', 'aire_acondicionado', 'jacuzzi', 'red_de_Gas', 'turco', 'porteria_24_7', 'sauna', 'calentador_de_agua', 'terraza', 'closet_de_linos', 'biblioteca', 'parqueadero_visitantes', 'gimnasio', 'piscina', 'salon_social', 'dispositivos_automatizacion', 'alarma']\n"
     ]
    }
   ],
   "source": [
    "# Lectura de carchivo con el nombre de las variables ya formateado\n",
    "format_columns = pd.read_csv(\"utils/format_columns.csv\", encoding='utf-8')\n",
    "format_columns = list(format_columns[\"columnas\"])\n",
    "\n",
    "# Lectura de archivo con el nombre de las columnas por cada grupo (general, caracteristicas, comodidades)\n",
    "# Consolidando los totales\n",
    "with open(\"utils/columns.json\", encoding=\"utf-8\") as columns:\n",
    "    columns = json.load(columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formateo de datos zona 1 centro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateando datos...\n",
      "Formato completo\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos del scraping\n",
    "with open(\"resultados_request/zona_1_centro.json\", encoding=\"utf-8\") as data_scraping:\n",
    "    data = json.load(data_scraping)\n",
    "proccess_data_scraping(data, format_columns, columns, 'resultados_procesar_datos/data_zona_1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formateo de datos zona 2 Poblado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateando datos...\n",
      "Formato completo\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos del scraping\n",
    "with open(\"resultados_request/zona_2_poblado.json\", encoding=\"utf-8\") as data_scraping:\n",
    "    data = json.load(data_scraping)\n",
    "proccess_data_scraping(data, format_columns, columns, 'resultados_procesar_datos/data_zona_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formateo de datos zona 3 Belén**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateando datos...\n",
      "Formato completo\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos del scraping\n",
    "with open(\"resultados_request/zona_3_belen.json\", encoding=\"utf-8\") as data_scraping:\n",
    "    data = json.load(data_scraping)\n",
    "proccess_data_scraping(data, format_columns, columns, 'resultados_procesar_datos/data_zona_3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formateo de datos zona 4 Laureles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateando datos...\n",
      "Formato completo\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos del scraping\n",
    "with open(\"resultados_request/zona_4_laureles.json\", encoding=\"utf-8\") as data_scraping:\n",
    "    data = json.load(data_scraping)\n",
    "proccess_data_scraping(data, format_columns, columns, 'resultados_procesar_datos/data_zona_4.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formateo de datos zona 5 San Antonio de Prado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateando datos...\n",
      "Formato completo\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos del scraping\n",
    "with open(\"resultados_request/zona_5_san_antonio.json\", encoding=\"utf-8\") as data_scraping:\n",
    "    data = json.load(data_scraping)\n",
    "proccess_data_scraping(data, format_columns, columns, 'resultados_procesar_datos/data_zona_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyesp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
